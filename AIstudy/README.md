# 결정트리
결정 트리(Decision Tree) : 데이터의 특성(feature)에 대한 '스무고개'같은 if-then 질문(규칙)을 반복하여 데이터를 분류하거나 값을 예측하는 지도학습 알고리즘

<원리>  
  1. 현재 데이터(부모 노드)를 균일하도록 쪼갤 수 있는 '특성', '경계값'을 찾는다. (정보이득 or 지니계수 사용)  
  2. 찾은 조건을 기준으로 서브 데이터 세트로 나눈다.  
  3. 위 과정을 반복한다.  
  4. 노드가 순수해지면 정지 or 하이퍼파라미터로 강제정지  

정보이득 : 1 - 엔트로피 지수 (서로 다른값이 섞여있으면 엔트로피 높음, 같은 값이 섞여있으면 엔트로피 낮음)  
지니계수 : 0수렴 -> 평등, 1수렴 -> 불평등

<특징>  
* 장점  
    1. 알고리즘이 쉽고 직관적  
    2. 시각화 표현 가능  
    3. 특별한 경우 제외 스케일링, 정규화 같은 전처리 작업 필요없다.  
* 단점  
    1. 과적합으로 정확도가 떨어짐 (보완: 트리의 크기를 사전에 제한하는 튜닝)

<파라미터>  
  - min_samples_split : 노드를 분할하기 위한 최소한의 샘플 데이터 수 (default = 2)  
  - min_samples_leaf  : 분할 후 왼쪽, 오른쪽 브랜치 노드에서 가져야할 최소한의 샘플 데이터 수

---
# k평균(mean)
K-평균(K-Means) : 데이터를 K개의 그룹으로 나누기 위해, 각 데이터가 가장 가까운 '중심점(평균)'에 속하도록 반복계산하여 군집을 형성하는 비지도 학습 알고리즘

<원리>
  1. 사용자가 정한 K개의 중심점(Centroid)를 데이터 공간에 무작위로 배치한다.
  2. 모든 데이터는 K개의 중심정 중 자신과 가장 가까운 중심점을 찾아, 그 중심점의 군집(cluster)으로 배정된다.
  3. 각 군집의 중심점을 그 군집에 새롭게 배정된 데이터들의 실제 평균(mean) 위치로 이동시킨다.
  4. 중심점이 이동했으므로 2번, 3번 과정을 중심점이 더 이상 움직이지 않을 때까지 반복한다.

<특징>
* 장점
    1. 알고리즘이 쉽고 직관적, 효율적
    2. 다양한 프로그래밍 언어로 구현 쉬움
    3. 데이터가 원형 또는 볼록한 형태로 뚜렷하게 구분될 때 좋은 군집 결과를 보여줌
* 단점
    1. 몇개의 군집(k)으로 나눌지 미리 정해줘야햠 (보완: 엘보우 방법 사용)
    2. 초기 중심점 민감성
    3. 클러스터 형태 한계: 길쭉하거나 불규칙한 모양 또는 도넛모양 군집은 잘 찾지 못함
    4. 이상치(Outlier)에 민감
    5. 데이터 스케일링 필요

<코드>
```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, random_state=42)
km.fit(data)
km.labels_ #군집 결과

#그림 출력
import matplotlib.pyplot as plt
fig, axs = plt.subplots(rows, cols, figsize=(10*10), squeeze=False)
axs[0].imshow(data[:], cmap='gray_r')
```
